{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instead of using statistical models like bigrams or trigrams, we will use a Multi-Layer Perceptron (MLP). This is because the array matrix required for storing letter combinations grows exponentially as 26^x, where x is the number of letter combinations. Since 26 represents the number of letters in the alphabet, this approach would require excessive computation and memory. Using a neural network like an MLP is a more efficient alternative.**\n",
    "\n",
    "<div align = 'center'>\n",
    "    <img src=\"./MLP_Arch.png\" width=\"500\">\n",
    "<div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1 Batch/Input Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Read in txt file\n",
    "words = open('names.txt', 'r').read().splitlines()\n",
    "words[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32033"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26, '.': 0}\n",
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n"
     ]
    }
   ],
   "source": [
    "# Convert the alphabet to a index integer using mapping \n",
    "chars = sorted(list(set(''.join(words))))\n",
    "# print(chars)\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0 \n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "print(stoi)\n",
    "print(itos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this following FUnction we are creating a dataset based off of indexs based on the previous mapping and storing them as tensors for each word. For example there is 5 words, so we will end up with a dataset of 5 empty arrays [. . .] and then a variation of all the words as shown in the print out of the funciton: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emma\n",
      "... -----> e\n",
      "..e -----> m\n",
      ".em -----> m\n",
      "emm -----> a\n",
      "mma -----> .\n",
      "olivia\n",
      "... -----> o\n",
      "..o -----> l\n",
      ".ol -----> i\n",
      "oli -----> v\n",
      "liv -----> i\n",
      "ivi -----> a\n",
      "via -----> .\n",
      "ava\n",
      "... -----> a\n",
      "..a -----> v\n",
      ".av -----> a\n",
      "ava -----> .\n",
      "isabella\n",
      "... -----> i\n",
      "..i -----> s\n",
      ".is -----> a\n",
      "isa -----> b\n",
      "sab -----> e\n",
      "abe -----> l\n",
      "bel -----> l\n",
      "ell -----> a\n",
      "lla -----> .\n",
      "sophia\n",
      "... -----> s\n",
      "..s -----> o\n",
      ".so -----> p\n",
      "sop -----> h\n",
      "oph -----> i\n",
      "phi -----> a\n",
      "hia -----> .\n",
      "32\n"
     ]
    }
   ],
   "source": [
    "# This is the block size for how big the input is going to be for the MLP\n",
    "block_size = 3\n",
    "\n",
    "X,y = [],[] # X is the input, y is the Label\n",
    "count = 5\n",
    "for w in words[:5]: \n",
    "    print(w)\n",
    "    context = [0] * block_size\n",
    "    count += len(w)\n",
    "    for ch in w + '.': \n",
    "        ix = stoi[ch]\n",
    "        # print('ch:', ch)\n",
    "        # print('ix', ix)\n",
    "        X.append(context)\n",
    "        # print('context', context)\n",
    "        y.append(ix)\n",
    "        print(''.join([itos[i] for i in context]),'----->', itos[ix])\n",
    "        context = context[1:] + [ix] \n",
    "        \n",
    "print(count)\n",
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function Summary:\n",
    "\n",
    "This function converts words into training data by mapping characters to indices and storing them as tensors.  \n",
    "\n",
    "- **Block Size (`3`)**: Defines how many previous characters form the input.  \n",
    "- **`X` (Input) & `y` (Labels)**:  \n",
    "  - `X` stores context windows of size 3.  \n",
    "  - `y` stores the next character for each window.  \n",
    "- **Process**:  \n",
    "  1. Start with an empty context (`[0, 0, 0]`).  \n",
    "  2. Slide through each word (plus `.` at the end).  \n",
    "  3. Store the context in `X` and the next character in `y`.  \n",
    "  4. Shift the context and repeat.  \n",
    "- **Example (`\"emma\"`)**:  \n",
    "  ```\n",
    "  ...  → e  \n",
    "  ..e  → m  \n",
    "  .em  → m  \n",
    "  emm  → a  \n",
    "  mma  → .  \n",
    "  ```  \n",
    "- Finally, `X` and `y` are converted to PyTorch tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 3]), torch.int64, torch.Size([32]), torch.int64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, X.dtype, Y.shape, Y.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0,  0,  0],\n",
       "         [ 0,  0,  5],\n",
       "         [ 0,  5, 13],\n",
       "         [ 5, 13, 13],\n",
       "         [13, 13,  1]]),\n",
       " tensor([ 5, 13, 13,  1,  0]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:5], Y[:5] # Printing the first 5 rows of the (X) (Y) tensor\n",
    "\n",
    "# X defines the sequence of characters, and Y defines the next character prediction in the sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As discussed before the dataset (X) resulted in 32 total rows, 5 empty intialization rows, and 27 variation rows that determine the next pattern. The block size determines how many characters will be remebered in the sequence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 2])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# C is the randome embedding matrix, with 27 rows and 2 columns\n",
    "C = torch.randn((27,2))\n",
    "\n",
    "# Embedding the input tensor X using the embedding matrix C\n",
    "embedding = C[X]\n",
    "embedding.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Embedding Summary**  \n",
    "\n",
    "- **Embeddings map characters to vectors** in a continuous space instead of treating them as separate symbols.  \n",
    "- **Initially random**, embeddings get optimized during training to capture character relationships.  \n",
    "- **Example:** A sequence `[0,0,5]` (where `5 = e`) retrieves vectors from an embedding matrix.  \n",
    "- **Training adjusts these vectors** so similar characters end up closer in space.  \n",
    "- **Benefit:** Helps the model understand patterns and improves predictions without manual rules.\n",
    "\n",
    "<div align = 'center'>\n",
    "    <img src=\"./2d_Vector_Embedding.png\" width=\"500\">\n",
    "<div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Hidden Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = torch.randn((6,100)) # This is the weights that will be input to the first layer\n",
    "b1 = torch.randn((100)) # This is the bias that will be input to the first layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We flatten the embeddings from `torch.Size([32, 3, 2]) → torch.Size([32, 6])` to make them compatible for weight multiplication. This is done using `torch.cat`, which concatenates the matrix into a single vector per sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 6])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([embedding[:, 0, :], embedding[:, 1, :], embedding[:, 2, :]], 1).shape # Inefficent way of doing it due to new memory being created"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the block size changes (e.g., from 3 to 5), the current approach would need manual updates. To avoid hardcoding, we use `torch.unbind`, which dynamically unravels the tensor, making the code more flexible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 6])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use of torch.unbind to \n",
    "# torch.unbind(embedding, 1)\n",
    "# This is equivalent to [embedding[:, 0, :], embedding[:, 1, :], embedding[:, 2, :]\n",
    "\n",
    "torch.cat(torch.unbind(embedding, 1), 1).shape\n",
    "# We get the same result as above and it is dynamic for block size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mehtod 2 More efficent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use `torch.view()` to reshape the tensor into any desired dimensions. For example, it allows us to convert a tensor from `torch.Size([32, 3, 2])` to `torch.Size([32, 6])` dynamically, making it adaptable to different input shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 6])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding.view(32,6) # This allows us to make the embedding matrix in a different shape\n",
    "embedding.view(32,6).shape\n",
    "# Testing similarity \n",
    "# embedding.view(32,6) == torch.cat(torch.unbind(embedding, 1), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4224,  0.0806, -0.0831,  ..., -2.2080,  2.0204,  4.3077],\n",
       "        [ 0.4335,  1.1128,  0.3140,  ..., -1.1884,  1.9151,  3.6025],\n",
       "        [-0.1176, -0.3584, -2.7237,  ..., -0.7868,  2.8766,  2.8699],\n",
       "        ...,\n",
       "        [-0.7861,  1.7516,  4.4418,  ..., -6.1124, -2.3208,  2.5561],\n",
       "        [ 3.0018,  2.2580,  9.5165,  ...,  0.7983, -0.6587,  1.3888],\n",
       "        [-0.1440,  2.3041, -0.7475,  ...,  2.3817,  1.4142, -0.1735]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# So to multiply the weights with inputs \n",
    "h = embedding.view(32,6) @ W1 + b1\n",
    "h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Multiplies** the **32 input vectors (each of size 6)** with **100 random intialized weights** from `W1 (6, 100)`.  \n",
    "- **Adds bias `b1`** for each of the 32 inputs, resulting in a final shape of `torch.Size([32, 100])`.  \n",
    "\n",
    "This transforms the input into a **100-dimensional hidden representation** for each sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of hardcoding 32 as the batch size in view(32,6), we can use -1 to make the code adaptable to any batch size. (-1) tells PyTorch to automatically infer the correct batch size based on the input. Input Size = The number of features per sample, Batch Size = The number of samples processed at once in a single forward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4224,  0.0806, -0.0831,  ..., -2.2080,  2.0204,  4.3077],\n",
       "        [ 0.4335,  1.1128,  0.3140,  ..., -1.1884,  1.9151,  3.6025],\n",
       "        [-0.1176, -0.3584, -2.7237,  ..., -0.7868,  2.8766,  2.8699],\n",
       "        ...,\n",
       "        [-0.7861,  1.7516,  4.4418,  ..., -6.1124, -2.3208,  2.5561],\n",
       "        [ 3.0018,  2.2580,  9.5165,  ...,  0.7983, -0.6587,  1.3888],\n",
       "        [-0.1440,  2.3041, -0.7475,  ...,  2.3817,  1.4142, -0.1735]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using -1 for dynamic block size\n",
    "h = embedding.view(-1,6) @ W1 + b1\n",
    "h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Softmax Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "W2 = torch.randn((100,27)) \n",
    "b2 = torch.randn((27))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 27])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = h @ W2 + b2 # Hidden layers are multiplied by weights of softmax layer and bias are added \n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = logits.exp() # Taking the exponential of the logits "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 27])\n",
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "# Normalizing the counts to get the probability of each character\n",
    "prob = counts / counts.sum(1, keepdim=True)\n",
    "print(prob.shape) # should return 32, 27, for 32 sameples and 27 characters\n",
    "\n",
    "print(prob[0].sum()) # The sume of the probabilities should be 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4.2885e-18, 5.5947e-30, 3.1483e-32, 5.7484e-09, 7.4170e-17, 2.6136e-10,\n",
       "        2.5237e-29, 2.6850e-21, 4.5206e-42, 5.2212e-18, 9.2960e-20, 1.8760e-35,\n",
       "        1.3772e-04, 2.2726e-41, 2.8684e-08, 6.2182e-36, 2.0051e-13, 2.5182e-25,\n",
       "        5.6042e-18, 1.0096e-21, 1.2250e-23, 1.4231e-17, 8.7150e-14, 2.4443e-13,\n",
       "        2.9141e-16, 1.0183e-22, 2.5092e-22, 2.1931e-28, 0.0000e+00, 2.7310e-01,\n",
       "        1.5461e-15, 4.5403e-27])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob[torch.arange(32), Y] # This is the probability of the correct character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(inf)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = - prob[torch.arange(32), Y].log().mean() # This is the loss function\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaned Up Full MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 3]), torch.Size([32]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, Y.shape # data set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1\n",
    "g = torch.Generator().manual_seed(494949) # Seed for reproducibility\n",
    "C = torch.rand((27,2), generator=g) # Random embedding matrix\n",
    "\n",
    "#Step 2\n",
    "W1 = torch.rand((6,100), generator=g) # Random weights for the first layer\n",
    "b1 = torch.rand((100), generator=g) # Random bias for the first layer\n",
    "\n",
    "#Step 3\n",
    "W2 = torch.rand((100,27), generator=g) # Random weights for the Softmax layer\n",
    "b2 = torch.rand((27), generator=g) # Random bias for the Softmax layer\n",
    "params = [C, W1, b1, W2, b2] # Parameters for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of Parameters:  3481\n"
     ]
    }
   ],
   "source": [
    "print('Total Number of Parameters: ', sum(p.nelement() for p in params)) # Total number of parameters in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.7902)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding = C[X] # Embedding the input tensor X using the embedding matrix C\n",
    "h = torch.tanh(embedding.view(-1,6) @ W1 + b1) # Hidden layer\n",
    "logits = h @ W2 + b2 # Logits\n",
    "counts = logits.exp() # Exponential of the logits\n",
    "prob = counts / counts.sum(1, keepdim=True) # Normalizing the counts\n",
    "loss = - prob[torch.arange(32), Y].log().mean() # Loss function\n",
    "loss "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of manually implementing the **cross-entropy loss function**, we use PyTorch’s built-in `torch.nn.functional.cross_entropy()`, which is more **optimized and efficient**.  \n",
    "\n",
    "Unlike a manual implementation, this **does not create extra tensors** for computation. Instead, it clusters operations together, making it **faster** and more **memory-efficient**.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in params: \n",
    "    p.requires_grad_() # Setting the requires_grad to True for all the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2718518376350403\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Training the model for 100 epochs\n",
    "for _ in range(1000): \n",
    "    # Forward pass\n",
    "    embedding = C[X] # Embedding the input tensor X using the embedding matrix C\n",
    "    h = torch.tanh(embedding.view(-1,6) @ W1 + b1) # Hidden layer\n",
    "    logits = h @ W2 + b2 # Logits\n",
    "    loss = F.cross_entropy(logits, Y) # Cross entropy loss function\n",
    "    \n",
    "    # Backward pass (Gradient descent)\n",
    "    for p in params: \n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    #update the parameters\n",
    "    for p in params: \n",
    "        p.data += -0.1 * p.grad\n",
    "    \n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even with just **5 words**, the loss dropped from **6.6207 → 0.2613**, but **not to 0**.  \n",
    "\n",
    "This happens because the **first character of each word is unpredictable**—the model starts with an **empty context**, meaning **any letter has an equal probability of being the first prediction**.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now Training on the full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the block size for how big the input is going to be for the MLP\n",
    "block_size = 3\n",
    "\n",
    "X,y = [],[] # X is the input, y is the Label\n",
    "\n",
    "for w in words: \n",
    "    context = [0] * block_size\n",
    "    for ch in w + '.': \n",
    "        ix = stoi[ch]\n",
    "        X.append(context)\n",
    "        y.append(ix)\n",
    "        context = context[1:] + [ix] \n",
    "        \n",
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([228146, 3]), torch.int64, torch.Size([228146]), torch.int64)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, X.dtype, Y.shape, Y.dtype\n",
    "# Now we have 228146 samples and 3 characters in each sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1\n",
    "g = torch.Generator().manual_seed(242424) # Seed for reproducibility\n",
    "C = torch.rand((27,2), generator=g) # Random embedding matrix\n",
    "\n",
    "#Step 2\n",
    "W1 = torch.rand((6,100), generator=g) # Random weights for the first layer\n",
    "b1 = torch.rand((100), generator=g) # Random bias for the first layer\n",
    "\n",
    "#Step 3\n",
    "W2 = torch.rand((100,27), generator=g) # Random weights for the Softmax layer\n",
    "b2 = torch.rand((27), generator=g) # Random bias for the Softmax layer\n",
    "params = [C, W1, b1, W2, b2] # Parameters for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in params: \n",
    "    p.requires_grad_() # Setting the requires_grad to True for all the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.896918296813965\n",
      "4.476920127868652\n",
      "3.65630841255188\n",
      "3.3312156200408936\n",
      "3.1505680084228516\n",
      "3.023245334625244\n",
      "2.9430642127990723\n",
      "2.9014089107513428\n",
      "2.8804824352264404\n",
      "2.8679776191711426\n",
      "2.859326124191284\n",
      "2.8529152870178223\n",
      "2.8479676246643066\n",
      "2.844022512435913\n",
      "2.8407864570617676\n",
      "2.838069200515747\n",
      "2.8357434272766113\n",
      "2.833721399307251\n",
      "2.8319427967071533\n",
      "2.8303616046905518\n",
      "2.828947067260742\n",
      "2.8276729583740234\n",
      "2.8265187740325928\n",
      "2.8254683017730713\n",
      "2.824509620666504\n",
      "2.823629379272461\n",
      "2.8228185176849365\n",
      "2.822068691253662\n",
      "2.82137131690979\n",
      "2.820718765258789\n",
      "2.8201065063476562\n",
      "2.8195266723632812\n",
      "2.818976402282715\n",
      "2.818449020385742\n",
      "2.8179426193237305\n",
      "2.8174526691436768\n",
      "2.816976308822632\n",
      "2.8165109157562256\n",
      "2.8160531520843506\n",
      "2.8156027793884277\n",
      "2.8151557445526123\n",
      "2.814711570739746\n",
      "2.8142688274383545\n",
      "2.8138256072998047\n",
      "2.8133809566497803\n",
      "2.8129336833953857\n",
      "2.812483310699463\n",
      "2.812028169631958\n",
      "2.8115668296813965\n",
      "2.811100959777832\n",
      "2.81062650680542\n",
      "2.8101444244384766\n",
      "2.809652805328369\n",
      "2.809152364730835\n",
      "2.8086416721343994\n",
      "2.808120012283325\n",
      "2.8075854778289795\n",
      "2.807039737701416\n",
      "2.8064796924591064\n",
      "2.805906057357788\n",
      "2.805316925048828\n",
      "2.8047120571136475\n",
      "2.804091453552246\n",
      "2.803452730178833\n",
      "2.8027961254119873\n",
      "2.8021209239959717\n",
      "2.8014261722564697\n",
      "2.8007113933563232\n",
      "2.7999746799468994\n",
      "2.7992165088653564\n",
      "2.7984347343444824\n",
      "2.797630786895752\n",
      "2.796802043914795\n",
      "2.7959492206573486\n",
      "2.7950711250305176\n",
      "2.7941668033599854\n",
      "2.79323673248291\n",
      "2.7922799587249756\n",
      "2.79129695892334\n",
      "2.7902867794036865\n",
      "2.7892489433288574\n",
      "2.788184881210327\n",
      "2.787095308303833\n",
      "2.7859785556793213\n",
      "2.7848379611968994\n",
      "2.7836711406707764\n",
      "2.782482147216797\n",
      "2.7812702655792236\n",
      "2.780038595199585\n",
      "2.778787136077881\n",
      "2.7775187492370605\n",
      "2.7762351036071777\n",
      "2.7749385833740234\n",
      "2.773630380630493\n",
      "2.7723140716552734\n",
      "2.770991802215576\n",
      "2.7696661949157715\n",
      "2.768338203430176\n",
      "2.767012596130371\n",
      "2.7656891345977783\n"
     ]
    }
   ],
   "source": [
    "# (Full-Batch Training)\n",
    "\n",
    "# Training the model for 100 epochs\n",
    "for _ in range(100): \n",
    "    # Forward pass\n",
    "    embedding = C[X] # Embedding the input tensor X using the embedding matrix C\n",
    "    h = torch.tanh(embedding.view(-1,6) @ W1 + b1) # Hidden layer\n",
    "    logits = h @ W2 + b2 # Logits\n",
    "    loss = F.cross_entropy(logits, Y) # Cross entropy loss function\n",
    "    print(loss.item())\n",
    "    # Backward pass (Gradient descent)\n",
    "    for p in params: \n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    #update the parameters\n",
    "    for p in params: \n",
    "        p.data += -0.1 * p.grad\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This process is taking too long because it performs a backward pass on every individual sample. A more efficient and commonly used strategy is to accumulate batches of samples and only run the backward pass on these mini-batches, rather than doing it for each sample or only once every epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full Training with Mini-Batch optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 3, 2, 1, 2, 4, 2, 1, 0, 4, 1, 1, 1, 0, 2, 1, 0, 0, 0, 3, 1, 4, 1, 4,\n",
       "        2, 2, 4, 1, 3, 3, 1, 4])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randint(0, 5, (32,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This function generates an array of 32 digits randomly that are between 0-5. We can use this funtionality on to produce minibatches from our dataset.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8847053050994873\n",
      "2.7311527729034424\n",
      "2.916980028152466\n",
      "2.8220932483673096\n",
      "2.950535297393799\n",
      "3.065720319747925\n",
      "2.5892081260681152\n",
      "2.7592172622680664\n",
      "2.979168176651001\n",
      "2.848599433898926\n",
      "2.8047873973846436\n",
      "2.714953660964966\n",
      "2.77398419380188\n",
      "2.7361083030700684\n",
      "3.3289566040039062\n",
      "2.8926572799682617\n",
      "2.781653881072998\n",
      "2.8973894119262695\n",
      "2.707707166671753\n",
      "2.7518696784973145\n",
      "2.8566346168518066\n",
      "2.3971445560455322\n",
      "2.745220899581909\n",
      "2.949542760848999\n",
      "3.1972897052764893\n",
      "2.9162373542785645\n",
      "2.510418176651001\n",
      "3.0068199634552\n",
      "2.8158912658691406\n",
      "2.6291110515594482\n",
      "2.6987574100494385\n",
      "2.972306966781616\n",
      "3.09956955909729\n",
      "2.897442102432251\n",
      "2.4859414100646973\n",
      "2.9215803146362305\n",
      "2.9073636531829834\n",
      "2.756169557571411\n",
      "2.9207074642181396\n",
      "2.5317189693450928\n",
      "2.701770305633545\n",
      "2.9801459312438965\n",
      "2.6830053329467773\n",
      "2.8954803943634033\n",
      "3.0044093132019043\n",
      "2.974919319152832\n",
      "2.7279038429260254\n",
      "3.0028884410858154\n",
      "2.5379905700683594\n",
      "2.782975435256958\n",
      "2.701331377029419\n",
      "2.6398251056671143\n",
      "2.9306960105895996\n",
      "2.486661672592163\n",
      "2.811873435974121\n",
      "2.8283963203430176\n",
      "2.643047571182251\n",
      "2.7480504512786865\n",
      "2.8816280364990234\n",
      "3.0097849369049072\n",
      "2.567183494567871\n",
      "2.766390323638916\n",
      "2.7410662174224854\n",
      "2.6254100799560547\n",
      "3.0129685401916504\n",
      "3.0102548599243164\n",
      "2.944028615951538\n",
      "2.86393404006958\n",
      "2.870905876159668\n",
      "2.7764363288879395\n",
      "2.549592971801758\n",
      "2.6125166416168213\n",
      "2.821192502975464\n",
      "2.629443407058716\n",
      "2.8736047744750977\n",
      "2.5488076210021973\n",
      "2.577894926071167\n",
      "2.648932695388794\n",
      "2.9573891162872314\n",
      "3.1269631385803223\n",
      "2.855693817138672\n",
      "2.5628881454467773\n",
      "2.9083309173583984\n",
      "2.5105457305908203\n",
      "2.8651039600372314\n",
      "2.8839969635009766\n",
      "2.669919490814209\n",
      "2.6414339542388916\n",
      "2.8764617443084717\n",
      "3.1436960697174072\n",
      "2.953122854232788\n",
      "2.58878493309021\n",
      "2.603717803955078\n",
      "2.5667595863342285\n",
      "2.639211893081665\n",
      "2.4959466457366943\n",
      "2.9128243923187256\n",
      "2.569159746170044\n",
      "2.9975788593292236\n",
      "2.6709401607513428\n"
     ]
    }
   ],
   "source": [
    "# (Mini-Batch Training)\n",
    "\n",
    "# Training the model for 100 epochs\n",
    "for _ in range(100): \n",
    "    \n",
    "    #Mini batch Construction\n",
    "    ix = torch.randint(0, X.shape[0], (32,)) # Randomly selecting 32 indexes from dataset storing it in ix\n",
    "    \n",
    "    # Forward pass\n",
    "    embedding = C[X[ix]] # using the randomly selected indexes to get the embedding matrix\n",
    "    h = torch.tanh(embedding.view(-1,6) @ W1 + b1) # Hidden layer\n",
    "    logits = h @ W2 + b2 # Logits\n",
    "    loss = F.cross_entropy(logits, Y[ix]) # Cross entropy loss function\n",
    "    print(loss.item())\n",
    "    # Backward pass (Gradient descent)\n",
    "    for p in params: \n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    #update the parameters\n",
    "    for p in params: \n",
    "        p.data += -0.1 * p.grad\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that now using mini-batches the speed dropped from 45 sec. to instant after the mini-batch ruling was applied this drastically reduced computation. \n",
    "\n",
    "- (Mini-Batch Training) significantly reduces computational cost per iteration by only computing gradients on a small subset of the data.\n",
    "- (Full-Batch Training) requires computing gradients over the entire dataset, making it slower and potentially impractical for large datasets.\n",
    "\n",
    "\n",
    "<div align = 'center'>\n",
    "    <img src=\"./Mini-batch_gradient_descent.webp\" width=\"500\">\n",
    "<div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Rate Optimization\n",
    "### Now we will look at the learning rate and how we can find the most optimal solution, common practice cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reset Variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the block size for how big the input is going to be for the MLP\n",
    "block_size = 3\n",
    "\n",
    "X,y = [],[] # X is the input, y is the Label\n",
    "\n",
    "for w in words: \n",
    "    context = [0] * block_size\n",
    "    for ch in w + '.': \n",
    "        ix = stoi[ch]\n",
    "        X.append(context)\n",
    "        y.append(ix)\n",
    "        context = context[1:] + [ix] \n",
    "        \n",
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(y)\n",
    "\n",
    "# Step 1\n",
    "g = torch.Generator().manual_seed(242424) # Seed for reproducibility\n",
    "C = torch.rand((27,2), generator=g) # Random embedding matrix\n",
    "\n",
    "#Step 2\n",
    "W1 = torch.rand((6,100), generator=g) # Random weights for the first layer\n",
    "b1 = torch.rand((100), generator=g) # Random bias for the first layer\n",
    "\n",
    "#Step 3\n",
    "W2 = torch.rand((100,27), generator=g) # Random weights for the Softmax layer\n",
    "b2 = torch.rand((27), generator=g) # Random bias for the Softmax layer\n",
    "params = [C, W1, b1, W2, b2] # Parameters for the model\n",
    "\n",
    "for p in params: \n",
    "    p.requires_grad_() # Setting the requires_grad to True for all the parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We determine the learing rate from the range 0f 10^-3 to 10^1\n",
    "lre = torch.linspace(-3, 0,10000)\n",
    "lrs = 10**lre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.4009056091308594"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (Mini-Batch Training)\n",
    "\n",
    "lri = [] # Stored list for the learning rate\n",
    "lossi = [] # Loss list of the model for the selected learning rate\n",
    "\n",
    "for i in range(10000): \n",
    "    \n",
    "    #Mini batch Construction\n",
    "    ix = torch.randint(0, X.shape[0], (32,))\n",
    "    \n",
    "    # Forward pass\n",
    "    embedding = C[X[ix]] \n",
    "    h = torch.tanh(embedding.view(-1,6) @ W1 + b1) \n",
    "    logits = h @ W2 + b2\n",
    "    loss = F.cross_entropy(logits, Y[ix]) \n",
    "\n",
    "    for p in params: \n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    lr = lrs[i]\n",
    "    for p in params: \n",
    "        p.data += -lr * p.grad # The -0.1 here is the learning rate\n",
    "    \n",
    "    lri.append(lre[i])\n",
    "    lossi.append(loss.item())\n",
    "\n",
    "loss.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(lri, lossi)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this we can see that the most optimal learning rate is for most optimal results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting Data - Prevent Overfitting\n",
    "### Best way to prevent overfitting of a LLM is to split the dataset into 3 for training split, dev/validation split, and test split: 80%, 10%, 10%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reset Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is same code as before but now optimized to call for each split dataset\n",
    "def build_dataset(words): \n",
    "    block_size = 3\n",
    "\n",
    "    X,y = [],[]\n",
    "\n",
    "    for w in words: \n",
    "        context = [0] * block_size\n",
    "        for ch in w + '.': \n",
    "            ix = stoi[ch]\n",
    "            X.append(context)\n",
    "            y.append(ix)\n",
    "            context = context[1:] + [ix] \n",
    "            \n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(y)\n",
    "    \n",
    "    return X,Y\n",
    "\n",
    "\n",
    "# Setting Model Parameters\n",
    "g = torch.Generator().manual_seed(242424) # Seed for reproducibility\n",
    "C = torch.rand((27,2), generator=g) # Random embedding matrix\n",
    "\n",
    "#Step 2\n",
    "W1 = torch.rand((6,100), generator=g) # Random weights for the first layer\n",
    "b1 = torch.rand((100), generator=g) # Random bias for the first layer\n",
    "\n",
    "#Step 3\n",
    "W2 = torch.rand((100,27), generator=g) # Random weights for the Softmax layer\n",
    "b2 = torch.rand((27), generator=g) # Random bias for the Softmax layer\n",
    "params = [C, W1, b1, W2, b2] # Parameters for the model\n",
    "\n",
    "for p in params: \n",
    "    p.requires_grad_() # Setting the requires_grad to True for all the parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words: 32033\n",
      "Training set range: (0 - 25626)\n",
      "Validation set range: (25626 - 28829)\n",
      "Testing set range: (28829 - 32033)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random.seed(1222)\n",
    "random.shuffle(words)\n",
    "\n",
    "n1 = int(len(words) * 0.8)\n",
    "n2 = int(len(words) * 0.9)\n",
    "\n",
    "print(f\"Total words: {len(words)}\")\n",
    "print(f\"Training set range: (0 - {n1})\")\n",
    "print(f\"Validation set range: ({n1} - {n2})\")\n",
    "print(f\"Testing set range: ({n2} - {len(words)})\")\n",
    "\n",
    "\n",
    "Xtrain, Ytrain = build_dataset(words[:n1])\n",
    "Xval, Yval = build_dataset(words[n1:n2])\n",
    "Xtest, Ytest = build_dataset(words[n2:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([182467, 3]),\n",
       " torch.Size([182467]),\n",
       " torch.Size([22922, 3]),\n",
       " torch.Size([22922]),\n",
       " torch.Size([22757, 3]),\n",
       " torch.Size([22757]))"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain.shape, Ytrain.shape, Xval.shape, Yval.shape, Xtest.shape, Ytest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.5406975746154785"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting the dataset/labels for the training set\n",
    "Dataset = Xtrain\n",
    "Labels = Ytrain\n",
    "\n",
    "\n",
    "for i in range(30000): \n",
    "    \n",
    "    #Mini batch Construction\n",
    "    ix = torch.randint(0, Dataset.shape[0], (32,))\n",
    "    \n",
    "    # Forward pass\n",
    "    embedding = C[Dataset[ix]] \n",
    "    h = torch.tanh(embedding.view(-1,6) @ W1 + b1) \n",
    "    logits = h @ W2 + b2\n",
    "    loss = F.cross_entropy(logits, Labels[ix]) \n",
    "\n",
    "    for p in params: \n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    lr = .1 # Learning rate\n",
    "    for p in params: \n",
    "        p.data += -lr * p.grad \n",
    "\n",
    "loss.item()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Validation set\n",
    "**Testing the model on validation set to see if model is overfitting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.3345775604248047"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = Xval\n",
    "labels = Yval   \n",
    "   \n",
    "embedding = C[dataset[ix]] \n",
    "h = torch.tanh(embedding.view(-1,6) @ W1 + b1) \n",
    "logits = h @ W2 + b2\n",
    "loss = F.cross_entropy(logits, labels[ix]) \n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.33760929107666"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = Xtest\n",
    "labels = Ytest   \n",
    "   \n",
    "embedding = C[dataset[ix]] \n",
    "h = torch.tanh(embedding.view(-1,6) @ W1 + b1) \n",
    "logits = h @ W2 + b2\n",
    "loss = F.cross_entropy(logits, labels[ix]) \n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here we can see that the loss of the Validation and Testing set, are roughly equal. This can indicate that the model is under fitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will need to optimize the network in order to perfrom better, we can do this by increasing the networks parameters to better fit the dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Increasing the Size of the N.N. \n",
    "**We can increase the Parameters of the Neural Network, inorder for it to better fit the data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Increasing the Neurons in the Hidden Layer (Step 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funtion for the datasets\n",
    "\n",
    "def build_dataset(words): \n",
    "    block_size = 3\n",
    "\n",
    "    X,y = [],[]\n",
    "\n",
    "    for w in words: \n",
    "        context = [0] * block_size\n",
    "        for ch in w + '.': \n",
    "            ix = stoi[ch]\n",
    "            X.append(context)\n",
    "            y.append(ix)\n",
    "            context = context[1:] + [ix] \n",
    "            \n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(y)\n",
    "    \n",
    "    return X,Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New # of Parameters: 10281\n"
     ]
    }
   ],
   "source": [
    "# Setting Model Parameters\n",
    "g = torch.Generator().manual_seed(242424) \n",
    "C = torch.rand((27,2), generator=g)\n",
    "\n",
    "#Step 2\n",
    "W1 = torch.rand((6,300), generator=g) # Changed hidden layer neurons from 100 to 300\n",
    "b1 = torch.rand((300), generator=g)  # Changed bias from 100 to 300\n",
    "\n",
    "#Step 3\n",
    "W2 = torch.rand((300,27), generator=g) # Changed softmax layer input from 100 to 300\n",
    "b2 = torch.rand((27), generator=g)\n",
    "params = [C, W1, b1, W2, b2] \n",
    "\n",
    "for p in params: \n",
    "    p.requires_grad_() \n",
    "    \n",
    "print('New # of Parameters:', sum(p.nelement() for p in params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.4074318408966064"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (Mini-Batch Training)\n",
    "dataset = Xtrain\n",
    "labels = Ytrain\n",
    "\n",
    "lossi = [] # Loss list of the model for the selected learning rate\n",
    "steps = [] \n",
    "\n",
    "for i in range(30000): \n",
    "    \n",
    "    #Mini batch Construction\n",
    "    ix = torch.randint(0, dataset.shape[0], (32,))\n",
    "    \n",
    "    # Forward pass\n",
    "    embedding = C[dataset[ix]] \n",
    "    h = torch.tanh(embedding.view(-1,6) @ W1 + b1) \n",
    "    logits = h @ W2 + b2\n",
    "    loss = F.cross_entropy(logits, labels[ix]) \n",
    "\n",
    "    for p in params: \n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    lr = .1\n",
    "    for p in params: \n",
    "        p.data += -lr * p.grad \n",
    "    \n",
    "    lossi.append(loss.item())\n",
    "    steps.append(i)\n",
    "\n",
    "loss.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(steps, lossi)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Xval\n",
    "labels = Yval   \n",
    "   \n",
    "embedding = C[dataset[ix]] \n",
    "h = torch.tanh(embedding.view(-1,6) @ W1 + b1) \n",
    "logits = h @ W2 + b2\n",
    "loss = F.cross_entropy(logits, labels[ix]) \n",
    "\n",
    "loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "makemore",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
